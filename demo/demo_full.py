# -*- coding: utf-8 -*-
"""FinalProject_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rUPrt0nMwxPncfN_wTBm8mGtKPBrL-Dr
"""

import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import zipfile

import torch
import seaborn as sns

"""Pull data from FNIST"""

# Pull data from FNIST

cwd = os.getcwd()
print(cwd)

# with zipfile.ZipFile('full_history.zip', 'r') as zip_ref:
#     zip_ref.extractall('/content')
amzn = pd.read_csv('/content/full_history/AMZN.csv')
amzn_np = amzn.to_numpy()

amzn_no_splits = []

for i in amzn_np:
  date = i[0]
  # Our dataset is off by exactly 1 years and 11 months.
  # AMZN stock split occurred on 06/06/2022
  if date < '2020-07-06' and date >= '1997-12-01':
    amzn_no_splits.append(i)

no_split_np = np.array(amzn_no_splits)
#get only the closing prices for the stock

close_data = no_split_np[:, -3]
close_data = np.flip(close_data)

stock_dates = no_split_np[:, 0]
stock_dates = np.flip(stock_dates)

# # First flatten each image into 784 to convert features from 3D -> 2D arrays
# train_features_flat = train_features.reshape((10000, 28 * 28))
# test_features_flat = test_features.reshape((1000, 28 * 28))

# Use standard scaler to normalize data
# scaler = StandardScaler()
# close_data_two_D = scaler.fit_transform(close_data.reshape(len(close_data),1))


Ms = MinMaxScaler()
close_data_two_D = Ms.fit_transform(close_data.reshape(len(close_data),1))

plt.figure(figsize=(10,5))
plt.plot(close_data_two_D)
plt.xlabel("days")
plt.ylabel("stock price")
plt.title("AMZN stock price from 01/01/2000 to 06/06/2022")
plt.show()

# number of days of stock prices
print("Length of the dataset:", num_datapoints, "days")

encoder_inputseq_len = 7 # num days
decoder_outputseq_len = 1 # num outputs (predicting next day)
testing_sequence_len = 7 # num days

test_size = int(0.5 * num_datapoints) # 40% of our data is for testing sequence

y_train = close_data_two_D[test_size:] # arr[start_pos:end_pos:skip]
y_test = close_data_two_D[:test_size]

y_train_dates = stock_dates[test_size:]
y_test_dates = stock_dates[:test_size]

plt.figure(figsize = (10, 5))
plt.plot(y_train, linewidth = 3, color = 'grey')

def generate_input_output_seqs(y, encoder_inputseq_len, decoder_outputseq_len, stride = 1, num_features = 1):

    L = y.shape[0] # Length of y

    # Calculate how many input/target sequences there will be based on the parameters and stride
    num_samples = (L - encoder_inputseq_len - decoder_outputseq_len) // stride + 1

    # Numpy zeros arrray to contain the input/target sequences
    # Note that they should be in (num_samples, seq_len, num_features/time step) format
    train_input_seqs = np.zeros([num_samples, encoder_inputseq_len, num_features])
    train_output_seqs = np.zeros([num_samples, decoder_outputseq_len, num_features])

    # Iteratively fill in train_input_seqs and train_output_seqs
    # See slide 17 of lab 7 to get an idea of how input_seqs and output_seqs look like
    for ff in np.arange(num_features):

        for ii in np.arange(num_samples):

            start_x = stride * ii
            end_x = start_x + encoder_inputseq_len
            train_input_seqs[ii, :, ff] = y[start_x:end_x, ff]

            start_y = stride * ii + encoder_inputseq_len
            end_y = start_y + decoder_outputseq_len
            train_output_seqs[ii, :, ff] = y[start_y:end_y, ff]

    return train_input_seqs, train_output_seqs

class Encoder(torch.nn.Module):

    def __init__(self, input_size, hidden_size, num_layers):

        super(Encoder, self).__init__()

        # Using LSTM for Encoder with batch_first = True

        self.lstm = torch.nn.LSTM(input_size = input_size, hidden_size = hidden_size,
                                  num_layers = num_layers,
                                  batch_first = True)

    def forward(self, input_seq, hidden_state):

        lstm_out, hidden = self.lstm(input_seq, hidden_state)

        return lstm_out, hidden

class Decoder(torch.nn.Module):

    def __init__(self, input_size, hidden_size, output_size, num_layers):

        super(Decoder, self).__init__()

        # Using LSTM for Decoder with batch_first = True
        # fc_decoder for converting hidden states -> single number

        self.lstm = torch.nn.LSTM(input_size = input_size, hidden_size = hidden_size,
                                  num_layers = num_layers,
                                  batch_first = True)

        self.fc_decoder = torch.nn.Linear(hidden_size, output_size)

    def forward(self, input_seq, encoder_hidden_states):

        lstm_out, hidden = self.lstm(input_seq, encoder_hidden_states)
        output = self.fc_decoder(lstm_out)

        return output, hidden

class Encoder_Decoder(torch.nn.Module):

    def __init__(self, input_size, hidden_size, decoder_output_size, num_layers):

        # Combine Encoder and Decoder classes into one

        super(Encoder_Decoder, self).__init__()

        self.Encoder = Encoder(input_size = input_size, hidden_size = hidden_size,
                               num_layers = num_layers)

        self.Decoder = Decoder(input_size = input_size, hidden_size = hidden_size,
                               output_size = decoder_output_size, num_layers = num_layers)

def get_batches(data, batchsize, encoder_inputseq_len = 7, decoder_outputseq_len = 1, ):
  train_input_seqs, train_output_seqs = generate_input_output_seqs(y = data,
                                                                 encoder_inputseq_len = encoder_inputseq_len,
                                                                 decoder_outputseq_len = decoder_outputseq_len,
                                                                 stride = 1,
                                                                 num_features = 1)

  # Check the dimensions of encoder input seqs and decoder output seqs
  print("Encoder Training Inputs Shape: ", train_input_seqs.shape)
  print("Decoder Training Outputs Shape: ", train_output_seqs.shape)

  # Convert training dataset into torch tensors
  training_input_seqs = torch.from_numpy(train_input_seqs).float()
  training_output_seqs = torch.from_numpy(train_output_seqs).float()

  # Split the training dataset to mini-batches
  # Skipping the last mini-batch since its size can be smaller than the set batchsize
  train_batches_features = torch.split(training_input_seqs, batchsize)[:-1]
  train_batches_targets = torch.split(training_output_seqs, batchsize)[:-1]

  # Total number of mini-batches in the training set
  batch_split_num = len(train_batches_features)

  return train_batches_features, train_batches_targets, batch_split_num

def train_LSTM(epochs, batch_split_num, train_batches_features, train_batches_targets, model, loss_func, optimizer, num_features):
  train_loss_list = []
  for epoch in range(epochs): # For each epoch

      for k in range(batch_split_num): # For each mini_batch

          # initialize hidden states to Encoder
          hidden_state = None

          # initialize empty torch tensor array to store decoder output sequence
          decoder_output_seq = torch.zeros(batchsize, decoder_outputseq_len, num_features)

          # empty gradient buffer
          optimizer.zero_grad()

          # Feed k-th mini-batch for encoder input sequences to encoder with hidden state
          encoder_output, encoder_hidden = model.Encoder(train_batches_features[k], hidden_state)
          # Re-define the resulting encoder hidden states as input hidden states to decoder
          decoder_hidden = encoder_hidden

          # Initial input to decoder is last timestep feature from the encoder input sequence
          decoder_input = train_batches_features[k][:, -1, :]
          # The extracted feature is 2D so need to add additional 3rd dimension
          # to conform to (sample size, seq_len, # of features)
          decoder_input = torch.unsqueeze(decoder_input, 2)

          # Populating the decoder output sequence
          for t in range(decoder_outputseq_len): # for each timestep in output sequence

              # Feed in the decoder_input and decoder_hidden to Decoder, get new output and hidden states
              decoder_output, decoder_hidden = model.Decoder(decoder_input, decoder_hidden)

              # Populate the corresponding timestep in decoder output sequence
              decoder_output_seq[:, t, :] = torch.squeeze(decoder_output, 2)

              # We are using teacher forcing so using the groundtruth training target as the next input
              decoder_input = train_batches_targets[k][:, t, :]

              # The extracted feature is 2D so need to add additional 3rd dimension
              # to conform to (sample size, seq_len, # of features)
              decoder_input = torch.unsqueeze(decoder_input, 2)

          # Compare the predicted decoder output sequence aginast the target sequence to compute the MSE loss
          loss = loss_func(torch.squeeze(decoder_output_seq), torch.squeeze(train_batches_targets[k]))

          # Save the loss
          train_loss_list.append(loss.item())

          # Backprop
          loss.backward()

          # Update the RNN
          optimizer.step()

      print("Averaged Training Loss for Epoch ", epoch,": ", np.mean(train_loss_list[-batch_split_num:]))

  return train_loss_list

# Fix random seed
torch.manual_seed(42)

# Using input_size = 1 (# of features to be fed to RNN per timestep)
# Using decoder_output_size = 1 (# of features to be output by Decoder RNN per timestep)
Encoder_Decoder_RNN = Encoder_Decoder(input_size = 1, hidden_size = 15,
                                      decoder_output_size = 1, num_layers = 1)

# Define learning rate + epochs
learning_rate = 0.0005
epochs = 20

# Define batch size and num_features/timestep (this is simply the last dimension of train_output_seqs)
batchsize = 5
num_features = 1 # train_output_seqs.shape[2]

# Define loss function/optimizer
loss_func = torch.nn.MSELoss()
optimizer = torch.optim.Adam(Encoder_Decoder_RNN.parameters(), lr=learning_rate)

print(Encoder_Decoder_RNN)

train_batches_features, train_batches_targets, batch_split_num = get_batches(y_train.reshape((-1, 1)), batchsize = batchsize)

# - bias
train_loss_list = train_LSTM(epochs, batch_split_num, train_batches_features, train_batches_targets, Encoder_Decoder_RNN, loss_func, optimizer, num_features)

plt.figure(figsize = (12, 7))

plt.plot(np.convolve(train_loss_list, np.ones(100), 'valid') / 100,
         linewidth = 3, label = 'Rolling Averaged Training Loss')
plt.ylabel("training loss")
plt.xlabel("Iterations")
plt.title("Training Loss for Predicting Stock Price")
plt.legend()
sns.despine()
plt.savefig('PredictedStockPriceLSTMTrainingLoss.png')
plt.show()

# Convert test sequence to tensor

def predict_stock_price(truth_seq, model, test_size, bias, amplitude, num_features=1, encoder_inputseq_len=7, decoder_outputseq_len=1):

  #print(y_test)

  # initialize empty torch tensor array to store decoder output sequence
  # This should be the same size as the test sequence
  decoder_output_seq = torch.zeros(test_size, num_features)

  # First n-datapoints in decoder output sequence = First n-datapoints in ground truth test sequence
  # n = encoder_input_seq_len
  decoder_output_seq[:encoder_inputseq_len] = truth_seq[:encoder_inputseq_len]

  # Initialize index for prediction
  pred_start_ind = 0

  # Activate no_grad() since we aren't performing backprop
  with torch.no_grad():

      # Loop continues until the RNN prediction reaches the end of the testing sequence length
      while pred_start_ind + encoder_inputseq_len + decoder_outputseq_len < test_size:

          # initialize hidden state for encoder
          hidden_state = None

          # Define the input to encoder
          input_test_seq = truth_seq[pred_start_ind:pred_start_ind + encoder_inputseq_len]
          # Add dimension to first dimension to keep the input (sample_size, seq_len, # of features/timestep)
          input_test_seq = torch.unsqueeze(input_test_seq, 0)

          # Feed the input to encoder and set resulting hidden states as input hidden states to decoder
          encoder_output, encoder_hidden = model.Encoder(input_test_seq, hidden_state)
          decoder_hidden = encoder_hidden

          # Initial input to decoder is last timestep feature from the encoder input sequence
          decoder_input = input_test_seq[:, -1, :]
          # Add dimension to keep the input (sample_size, seq_len, # of features/timestep)
          decoder_input = torch.unsqueeze(decoder_input, 2)

          # Populate decoder output sequence
          for t in range(decoder_outputseq_len):

              # Generate new output for timestep t
              decoder_output, decoder_hidden = model.Decoder(decoder_input, decoder_hidden)
              # Populate the corresponding timestep in decoder output sequence
              decoder_output_seq[pred_start_ind + encoder_inputseq_len + t] = (torch.squeeze(decoder_output) * amplitude) - bias
              # Use the output of the decoder as new input for the next timestep
              decoder_input = decoder_output

          # Update pred_start_ind
          pred_start_ind += decoder_outputseq_len

      return decoder_output_seq

test_input_seq = torch.from_numpy(np.array(y_test,dtype="float64")).float()

print(test_input_seq.shape)
test_input_seq= test_input_seq.reshape((-1,1))

print(test_input_seq.shape)


decoder_output_seq = predict_stock_price(test_input_seq, Encoder_Decoder_RNN, test_size, .036, 1)

# Compare the RNN prediction (decoder output sequence) vs the ground truth sequence
plt.figure(figsize = (10, 5))

plt.plot(test_input_seq, linewidth = 3, label = 'GroundTruth')
plt.plot(decoder_output_seq, linewidth = 3, label = 'RNN Predicted')
plt.title('RNN Predicted vs GroundTruth for Predicting Stock Price')
plt.xlabel("days")
plt.ylabel("stock price (normalized)")
plt.legend()
plt.savefig('LSTMPredictedVSGroundTruth.png')

#Calculate residuals

test_size_residuals = int(0.5 * y_test.size) # 20% of our data is for testing sequence

residuals_train_dates = y_test_dates[test_size_residuals:]
residuals_test_dates = y_test_dates[:test_size_residuals]

y_2_train = y_test[test_size_residuals:] # arr[start_pos:end_pos:skip]

y_2_test = y_test[:test_size_residuals]

decoder_output_train = decoder_output_seq[test_size_residuals:]

decoder_output_test = decoder_output_seq[:test_size_residuals]

#Residuals: Difference between the predicted stock price and the actual stock price

residuals_train = torch.from_numpy(np.array(y_2_train,dtype="float64")).float() - decoder_output_train

residuals_test = torch.from_numpy(np.array(y_2_test,dtype="float64")).float() - decoder_output_test

print(residuals_train.shape)

print(residuals_test.shape)

# Fix random seed
torch.manual_seed(42)

# Using input_size = 1 (# of features to be fed to RNN per timestep)
# Using decoder_output_size = 1 (# of features to be output by Decoder RNN per timestep)
Encoder_Decoder_RNN_Residual = Encoder_Decoder(input_size = 1, hidden_size = 15,
                                      decoder_output_size = 1, num_layers = 1)

# Define learning rate + epochs
learning_rate = 0.0005
epochs = 10

# Define batch size and num_features/timestep (this is simply the last dimension of train_output_seqs)
batchsize = 5
num_features = 1 # train_output_seqs.shape[2]

# Define loss function/optimizer
loss_func = torch.nn.MSELoss()
optimizer = torch.optim.Adam(Encoder_Decoder_RNN_Residual.parameters(), lr=learning_rate)

print(Encoder_Decoder_RNN_Residual)

train_batches_features, train_batches_targets, batch_split_num = get_batches(residuals_train.reshape((-1, 1)), batchsize = batchsize)

train_loss_list = train_LSTM(epochs, batch_split_num, train_batches_features, train_batches_targets, Encoder_Decoder_RNN_Residual, loss_func, optimizer, num_features)

plt.figure(figsize = (12, 7))

plt.plot(np.convolve(train_loss_list, np.ones(100), 'valid') / 100,
         linewidth = 3, label = 'Rolling Averaged Training Loss')
plt.ylabel("training loss")
plt.xlabel("Iterations")
plt.title("Training Loss for Residuals")
plt.legend()
sns.despine()
plt.savefig('ResidualsLSTMTrainingLoss.png')
plt.show()

#Use the last 20% as data to detect anomalies

test_input_seq = torch.from_numpy(np.array(residuals_test,dtype="float64")).float()

print(test_input_seq.shape)
test_input_seq= test_input_seq.reshape((-1,1))

print(test_input_seq.shape)


decoder_output_seq = predict_stock_price(test_input_seq, Encoder_Decoder_RNN_Residual, test_size_residuals, .0485, -80)

# Compare the RNN prediction (decoder output sequence) vs the ground truth sequence
plt.figure(figsize = (10, 5))

plt.plot(test_input_seq, linewidth = 3, label = 'GroundTruth')
plt.plot(decoder_output_seq, linewidth = 3, label = 'RNN Predicted Residuals')
plt.title('RNN Predicted Residuals vs GroundTruth')
plt.xlabel("days")
plt.ylabel("residuals (normalized)")
plt.legend()
plt.savefig('LSTMPredictedResidualsVSGroundTruth.png')

#Anomaly Detection
#Predicted residuals - actual residuals

residual_differences = decoder_output_seq - test_input_seq

# print(residual_differences)
# print(residuals_test_dates)

print(residuals_test_dates.shape)
print(residual_differences.shape)
print(torch.max(residual_differences))
print(torch.min(residual_differences))

absolute_residual_differences = torch.abs(residual_differences)

condition = (absolute_residual_differences > .007)

filtered_tensor = torch.where(condition, 1, 0)

print(filtered_tensor.shape)

filtered_array = filtered_tensor.numpy()


anomaly_dates = []


for i in range(len(filtered_array)):
  if filtered_array[i] == 1:
    anomaly_dates.append(residuals_test_dates[i])


print(anomaly_dates)

# Our dataset is off by exactly 1 years and 11 months.
  # AMZN stock split occurred on 06/06/2022
  # if date < '2020-07-06'

#Output, 4 dates: ['1997-12-11', '1999-01-13', '1999-01-14', '1999-04-29']. Lets see what happened on these dates!

#'1999-11-11': No idea what happened here lol
#'2000-12-13': Amazon reports its first profitable quarter in its history!
#'2000-12-13': Amazon reports its first profitable quarter in its history!
#'2001-03-29': Amazon reports its first profitable quarter in its history!

# I also want to predict use the models to predict the

#Post stock split calculation
amzn_post_split = []
y_test_dates = []
for i in amzn_np:
  date = i[0]
  # Our dataset is off by exactly 1 years and 11 months.
  # AMZN stock split occurred on 06/06/2022
  if date > '2020-08-06':
    amzn_post_split.append(i)
    y_test_dates.append(date)

amzn_post_split = np.array(amzn_post_split)
#get only the closing prices for the stock

amzn_post_split = amzn_post_split[:, -3]
amzn_post_split = np.flip(amzn_post_split)

y_test_dates = np.flip(y_test_dates)
scaled_amzn_post_split = Ms.fit_transform(amzn_post_split.reshape(len(amzn_post_split),1))

plt.figure(figsize=(10,5))
plt.plot(scaled_amzn_post_split)
plt.xlabel("days")
plt.ylabel("stock price")
plt.title("AMZN stock price from 06/06/2022 to now")
plt.show()


test_input_seq = torch.from_numpy(np.array(scaled_amzn_post_split,dtype="float64")).float()

print(test_input_seq.shape)
test_input_seq= test_input_seq.reshape((-1,1))

print(test_input_seq.shape)
test_size = test_input_seq.shape[0]
print(test_size)

decoder_output_seq = predict_stock_price(test_input_seq, Encoder_Decoder_RNN, test_size, .036, 1)

plt.figure(figsize = (10, 5))
plt.plot(test_input_seq, linewidth = 3, label = 'GroundTruth')
plt.plot(decoder_output_seq, linewidth = 3, label = 'RNN Predicted')
plt.title('RNN Predicted vs GroundTruth for Predicting Stock Price')
plt.xlabel("days")
plt.ylabel("stock price (normalized)")
plt.legend()

#Calculate residuals

residuals_test_dates = y_test_dates

#Residuals: Difference between the predicted stock price and the actual stock price
residuals_test = torch.from_numpy(np.array(test_input_seq,dtype="float64")).float() - decoder_output_seq

print(residuals_test.shape)

test_input_seq = torch.from_numpy(np.array(residuals_test,dtype="float64")).float()

print(test_input_seq.shape)
test_input_seq= test_input_seq.reshape((-1,1))

print(test_input_seq.shape)


decoder_output_seq = predict_stock_price(test_input_seq, Encoder_Decoder_RNN_Residual, test_size, .0485, -80)

plt.figure(figsize = (10, 5))

plt.plot(test_input_seq, linewidth = 3, label = 'GroundTruth')
plt.plot(decoder_output_seq, linewidth = 3, label = 'RNN Predicted Residuals')
plt.title('RNN Predicted Residuals vs GroundTruth')
plt.xlabel("days")
plt.ylabel("residuals (normalized)")
plt.legend()
plt.savefig('LSTMPredictedResidualsVSGroundTruth.png')

residual_diff_percentages = []



residual_diff =  torch.abs(decoder_output_seq - test_input_seq)





# print(len(y_test_dates))
# print(differences.shape)
print(torch.max(residual_diff))
print(torch.min(residual_diff))

# print(residual_diff_percentages)

condition = (residual_diff > 0.15)

filtered_tensor = torch.where(condition, 1, 0)

print(filtered_tensor.shape)

filtered_array = filtered_tensor.numpy()


anomaly_dates = []


for i in range(len(filtered_array)):
  if filtered_array[i] == 1:
    print(amzn_post_split[i])
    anomaly_dates.append(residuals_test_dates[i])

#'2023-09-28', '2023-01-05', '2022-07-06', '2021-09-21'

print(len(anomaly_dates))
print(anomaly_dates)
# print(anomaly_dates)

print(len(residual_diff))
# print(torch.max(residual_diff_percentages))

# from google.colab import drive
# drive.mount('/content/drive')

# # Pull data from FNIST

# cwd = os.getcwd()
# print(cwd)

# amzn = pd.read_csv('drive/MyDrive/EE596/Practical Deep Learning/Project/full_history/AMZN.csv')
# amzn_np = amzn.to_numpy()

# amzn_full = []
# amzn_no_splits = []

# for i in amzn_np:
#   date = i[0]
#   # Our dataset is off by exactly 2 years and 1 month.
#   # AMZN stock split occurred on 06/06/2022
#   if date < '2020-07-06' and date >= '1997-12-01':
#     amzn_no_splits.append(i)
#   amzn_full.append(i)

# no_split_np = np.array(amzn_no_splits)
# full_amzn_np = np.array(amzn_full)
# #get only the closing prices for the stock

# close_data = no_split_np[:, -3]
# close_data = np.flip(close_data)

# close_data_full = full_amzn_np[:, -3]
# close_data_full = np.flip(close_data_full)

plt.figure(figsize=(10,5))
plt.plot(close_data)
plt.xlabel("days")
plt.ylabel("stock price")
plt.title("AMZN stock price from 01/01/2000 to 06/06/2022")


num_datapoints = close_data.size
plt.axvline(x=num_datapoints*0.5, color='r', label="50% of data", linestyle='--')
plt.axvline(x=num_datapoints*0.25, color='g', label="25% of data", linestyle='--')
plt.legend()
plt.savefig("AMZN_stock_price_trimmed.png")

plt.show()

plt.figure(figsize=(10,5))
plt.plot(close_data_full)
plt.xlabel("days")
plt.ylabel("stock price")
plt.title("AMZN stock price")

plt.savefig("AMZN_stock_price_full.png")

plt.show()

